{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bringing modularity and code reuse to Spark\n",
    "\n",
    "Spark does not let one define arbitrary functions and reuse them at will. In this example, we show how to decompose a problem into a set of simpler primitive functions, that nevertheless perform arbitrary operations that would not be allowed in Spark.\n",
    "\n",
    "We are going to build a function that exemplifies the birthday paradox: given a set of birthdates, it will returns the number of people who happen to share a birthdate with someone else. This is easy to express using joins. This function takes a dataset or a column as input (the birth dates) and returns a single number (the number of people who share the same birth day). This is an aggregation function! Our urge is of course to use it then in a different setting such as in a group, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining with structured transforms\n",
    "import karps as ks\n",
    "import karps.functions as f\n",
    "from karps.display import show_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a session to be done with it\n",
    "s = ks.session(\"demo2e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremely small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/employees@org.spark.DistributedLiteral:{company_name:string, employee_name:string, dob:string}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees = ks.dataframe([\n",
    "    (\"ACME\", \"John\", \"12/01\"),\n",
    "    (\"ACME\", \"Kate\", \"09/04\"),\n",
    "    (\"ACME\", \"Albert\", \"09/04\"),\n",
    "    (\"Databricks\", \"Ali\", \"09/04\"),\n",
    "], schema=[\"company_name\", \"employee_name\", \"dob\"],\n",
    "   name=\"employees\")\n",
    "employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, here is the definition of the birthday paradox function. It is pretty simple code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of people who share a birthday date with someone else.\n",
    "# Takes a column of data containing birthdates.\n",
    "def paradoxal_count(c):\n",
    "    with ks.scope(\"p_count\"): # Make it pretty:\n",
    "        g = c.groupby(c).agg({'num_employees': f.count}, name=\"agg_count\")\n",
    "        s = f.sum(g.num_employees[g.num_employees>=2], name=\"paradoxical_employees\")\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can try it out on a simple dataset. Unlike spark, note that columns and datasets are the same for data with single elements.\n",
    "\n",
    "It correctly found that 2 people share the same January 1st birth date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, int_value: 2\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ks.dataframe([\"1/1\", \"3/1\", \"1/1\"])\n",
    "s.run(paradoxal_count(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this nice function, let's use against each of the companies in our dataset.\n",
    "\n",
    "Notice that you can directly plug the function, no need to do translation, etc. This is impossible to do in Spark for complex functions like this one.\n",
    "\n",
    "We get at the end a daframe with the name of the company and the number of employees that share the same birthdate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/agg_post10@org.spark.StructuredTransform:{company_name:string, paradoxical_employees:int}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now use this to group by companies:\n",
    "res = (employees.dob\n",
    "       .groupby(employees.company_name)\n",
    "       .agg({\n",
    "           \"paradoxical_employees\": paradoxal_count\n",
    "       }))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to collect and see the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/collect_list11!org.spark.StructuredReduce:[{company_name:string, paradoxical_employees:int}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = f.collect(res)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<karps.computation.Computation at 0x10cbe80f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp = s.compute(o)\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's look under the hood to see how this gets translated.\n",
    "\n",
    "The transformation is defined using two nested first-orderd functions, that get collected using the `FunctionalShuffle` operation called `shuffle17`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1000px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.6433116666549593&quot;).pbtxt = 'node {\\n  name: &quot;p_count/placeholder9&quot;\\n  op: &quot;org.spark.Placeholder&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;string&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;locality: DISTRIBUTED data_type { basic_type: STRING }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;placeholder8&quot;\\n  op: &quot;org.spark.Placeholder&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;string&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;locality: DISTRIBUTED data_type { basic_type: STRING }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;employees&quot;\\n  op: &quot;org.spark.DistributedLiteral&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{company_name:string employee_name:string dob:string}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;cell {  array_value {    values {      struct_value {        values { string_value: .ACME. }        values { string_value: .John. }        values { string_value: .12/01. }      }    }    values {      struct_value {        values { string_value: .ACME. }        values { string_value: .Kate. }        values { string_value: .09/04. }      }    }    values {      struct_value {        values { string_value: .ACME. }        values { string_value: .Albert. }        values { string_value: .09/04. }      }    }    values {      struct_value {        values { string_value: .Databricks. }        values { string_value: .Ali. }        values { string_value: .09/04. }      }    }  }}cell_type {  array_type {    struct_type {      fields {        field_name: .company_name. field_type { basic_type: STRING }      }      fields {        field_name: .employee_name. field_type { basic_type: STRING }      }      fields { field_name: .dob. field_type { basic_type: STRING } }    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;agg_pre16&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;employees&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:string value:string}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .company_name. } field_name: .key. }    fields { extraction { path: .dob. } field_name: .value. }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/agg_pre12&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;placeholder8&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:string value:string}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { } field_name: .key. }    fields { extraction { } field_name: .value. }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/structured_transform10&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/placeholder9&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;string&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op { extraction { } }&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/count11&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;p_count/structured_transform10&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;int&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .count. inputs { } expected_type { basic_type: INT }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/agg_count&quot;\\n  op: &quot;org.spark.FunctionalShuffle&quot;\\n  input: &quot;p_count/agg_pre12&quot;\\n  input: &quot;p_count/placeholder9&quot;\\n  input: &quot;p_count/count11&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:string value:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/agg_post13&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/agg_count&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:string num_employees:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .key. } field_name: .key. }    fields { extraction { path: .value. } field_name: .num_employees. }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/filter_pre14&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/agg_post13&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{filter:bool value:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields {      function {        function_name: .greater_equal.        inputs { extraction { path: .num_employees. } }        inputs {          literal {            content { cell { int_value: 2 } cell_type { basic_type: INT } }          }        }        expected_type { basic_type: BOOL }      }      field_name: .filter.    }    fields { extraction { path: .num_employees. } field_name: .value. }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/filter15&quot;\\n  op: &quot;org.spark.Filter&quot;\\n  input: &quot;p_count/filter_pre14&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;int&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/paradoxical_employees&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;p_count/filter15&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;int&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .sum. inputs { } expected_type { basic_type: INT }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;shuffle17&quot;\\n  op: &quot;org.spark.FunctionalShuffle&quot;\\n  input: &quot;agg_pre16&quot;\\n  input: &quot;placeholder8&quot;\\n  input: &quot;p_count/paradoxical_employees&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:string value:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;agg_post18&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;shuffle17&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{company_name:string paradoxical_employees:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .key. } field_name: .company_name. }    fields {      extraction { path: .value. } field_name: .paradoxical_employees.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;collect_list19&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;agg_post18&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;[{company_name:string paradoxical_employees:int}]&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .collect_list.    inputs { }    expected_type {      array_type {        struct_type {          fields {            field_name: .company_name. field_type { basic_type: STRING }          }          fields {            field_name: .paradoxical_employees. field_type { basic_type: INT }          }        }      }    }  }}&quot;\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.6433116666549593&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_phase(comp, \"initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimization and flattening, the graph actually turns out to be a linear graph with a first shuffle, a filter, a second shuffle and then a final aggregate. You can click around to see how computations are being done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1000px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.009932481629656542&quot;).pbtxt = 'node {\\n  name: &quot;employees&quot;\\n  op: &quot;org.spark.DistributedLiteral&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{company_name:string employee_name:string dob:string}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;cell {  array_value {    values {      struct_value {        values { string_value: .ACME. }        values { string_value: .John. }        values { string_value: .12/01. }      }    }    values {      struct_value {        values { string_value: .ACME. }        values { string_value: .Kate. }        values { string_value: .09/04. }      }    }    values {      struct_value {        values { string_value: .ACME. }        values { string_value: .Albert. }        values { string_value: .09/04. }      }    }    values {      struct_value {        values { string_value: .Databricks. }        values { string_value: .Ali. }        values { string_value: .09/04. }      }    }  }}cell_type {  array_type {    struct_type {      fields {        field_name: .company_name. field_type { basic_type: STRING }      }      fields {        field_name: .employee_name. field_type { basic_type: STRING }      }      fields { field_name: .dob. field_type { basic_type: STRING } }    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;agg_pre16&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;employees&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:string value:string}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .company_name. } field_name: .key. }    fields { extraction { path: .dob. } field_name: .value. }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;placeholder8&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;agg_pre16&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_1:string} value:string}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields {      struct {        fields { extraction { path: .key. } field_name: .key_1. }      }      field_name: .key.    }    fields { extraction { path: .value. } field_name: .value. }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/agg_pre12&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;placeholder8&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_1:string} value:{key:string value:string}}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .key. } field_name: .key. }    fields {      struct {        fields { extraction { path: .value. } field_name: .key. }        fields { extraction { path: .value. } field_name: .value. }      }      field_name: .value.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/placeholder9&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/agg_pre12&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_2:string key_1:string} value:string}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields {      struct {        fields {          extraction { path: .key. path: .key_1. } field_name: .key_1.        }        fields {          extraction { path: .value. path: .key. } field_name: .key_2.        }      }      field_name: .key.    }    fields {      extraction { path: .value. path: .value. } field_name: .value.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/structured_transform10&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/placeholder9&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_2:string key_1:string} value:string}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .key. } field_name: .key. }    fields { extraction { path: .value. } field_name: .value. }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/count11&quot;\\n  op: &quot;org.spark.GroupedReduction&quot;\\n  input: &quot;p_count/structured_transform10&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_2:string key_1:string} value:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .count. inputs { } expected_type { basic_type: INT }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/agg_count&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/count11&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_1:string} value:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields {      struct {        fields {          extraction { path: .key. path: .key_1. } field_name: .key_1.        }      }      field_name: .key.    }    fields {      struct {        fields {          extraction { path: .key. path: .key_2. } field_name: .key.        }        fields { extraction { path: .value. } field_name: .value. }      }      field_name: .value.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/agg_post13&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/agg_count&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_1:string} value:{key:string num_employees:int}}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .key. } field_name: .key. }    fields {      struct {        fields {          extraction { path: .value. path: .key. } field_name: .key.        }        fields {          extraction { path: .value. path: .value. }          field_name: .num_employees.        }      }      field_name: .value.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/filter_pre14&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/agg_post13&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_1:string} value:{filter:bool value:int}}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .key. } field_name: .key. }    fields {      struct {        fields {          function {            function_name: .greater_equal.            inputs { extraction { path: .value. path: .num_employees. } }            inputs {              literal {                content { cell { int_value: 2 } cell_type { basic_type: INT } }              }            }            expected_type { basic_type: BOOL }          }          field_name: .filter.        }        fields {          extraction { path: .value. path: .num_employees. }          field_name: .value.        }      }      field_name: .value.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/filter15_kagg_filter&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/filter_pre14&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{filter:bool value:{key:{key_1:string} value:int}}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields {      extraction { path: .value. path: .filter. } field_name: .filter.    }    fields {      struct {        fields { extraction { path: .key. } field_name: .key. }        fields {          extraction { path: .value. path: .value. } field_name: .value.        }      }      field_name: .value.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/filter15&quot;\\n  op: &quot;org.spark.Filter&quot;\\n  input: &quot;p_count/filter15_kagg_filter&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_1:string} value:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;p_count/paradoxical_employees&quot;\\n  op: &quot;org.spark.GroupedReduction&quot;\\n  input: &quot;p_count/filter15&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:{key_1:string} value:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .sum. inputs { } expected_type { basic_type: INT }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;shuffle17&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;p_count/paradoxical_employees&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{key:string value:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields {      extraction { path: .key. path: .key_1. } field_name: .key.    }    fields { extraction { path: .value. } field_name: .value. }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;agg_post18&quot;\\n  op: &quot;org.spark.StructuredTransform&quot;\\n  input: &quot;shuffle17&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Distributed&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;{company_name:string paradoxical_employees:int}&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;col_op {  struct {    fields { extraction { path: .key. } field_name: .company_name. }    fields {      extraction { path: .value. } field_name: .paradoxical_employees.    }  }}&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;collect_list19&quot;\\n  op: &quot;org.spark.StructuredReduce&quot;\\n  input: &quot;agg_post18&quot;\\n  device: &quot;/spark:0&quot;\\n  attr {\\n    key: &quot;locality&quot;\\n    value {\\n      s: &quot;Local&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;type&quot;\\n    value {\\n      s: &quot;[{company_name:string paradoxical_employees:int}]&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;zextra&quot;\\n    value {\\n      s: &quot;agg_op {  op {    function_name: .collect_list.    inputs { }    expected_type {      array_type {        struct_type {          fields {            field_name: .company_name. field_type { basic_type: STRING }          }          fields {            field_name: .paradoxical_employees. field_type { basic_type: INT }          }        }      }    }  }}&quot;\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.009932481629656542&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_phase(comp, \"final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{company_name:string, paradoxical_employees:int}], array_value {\n",
       "  values {\n",
       "    struct_value {\n",
       "      values {\n",
       "        string_value: \"ACME\"\n",
       "      }\n",
       "      values {\n",
       "        int_value: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conclusion, with Karps, you can take _any_ reasonable function and reuse it in arbitrary ways in a functional manner, in a type-safe manner. Karps will write for you the complex SQL queries that you would have to write by hand. All errors are detected well before the actual runtime, which greatly simplifies the debugging.\n",
    "\n",
    "Laziness and structured transforms bring to Spark some fundamental characteristics such as modularity, reusability, better testing and fast-fail comprehensive error checking, on top of automatic performance optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
